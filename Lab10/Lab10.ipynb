{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61832beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemented from scratch:\n",
    "# 1. Softmax Logistic Regression for MNIST dataset\n",
    "# 2. Naive Bayes for Bank dataset\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Cell 1: Import Libraries\n",
    "\n",
    "\n",
    "# %%\n",
    "import os, time, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4897ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define Metric Functions\n",
    "\n",
    "def confusion_matrix_multiclass(y_true, y_pred, labels=None):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    if labels is None:\n",
    "        labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    label_to_idx = {l: i for i, l in enumerate(labels)}\n",
    "    cm = np.zeros((len(labels), len(labels)), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[label_to_idx[t], label_to_idx[p]] += 1\n",
    "    return cm, labels\n",
    "\n",
    "\n",
    "def precision_recall_f1_from_cm(cm):\n",
    "    tp = np.diag(cm)\n",
    "    fp = cm.sum(axis=0) - tp\n",
    "    fn = cm.sum(axis=1) - tp\n",
    "    precision = np.where(tp + fp == 0, 0, tp / (tp + fp))\n",
    "    recall = np.where(tp + fn == 0, 0, tp / (tp + fn))\n",
    "    f1 = np.where(precision + recall == 0, 0, 2 * precision * recall / (precision + recall))\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def print_metrics_from_labels(y_true, y_pred, labels=None, name=\"Model\"):\n",
    "    cm, labels = confusion_matrix_multiclass(np.array(y_true), np.array(y_pred), labels=labels)\n",
    "    precision, recall, f1 = precision_recall_f1_from_cm(cm)\n",
    "    acc = cm.diagonal().sum() / cm.sum()\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    for lab, p, r, f in zip(labels, precision, recall, f1):\n",
    "        print(f\"Label {lab}: P={p:.4f}, R={r:.4f}, F1={f:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d4d61fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 42000, Testing samples: 28000\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load MNIST Dataset\n",
    "\n",
    "MNIST_TRAIN_PATH = '/mnt/data/train.csv' if os.path.exists('/mnt/data/train.csv') else 'train.csv'\n",
    "MNIST_TEST_PATH  = '/mnt/data/test.csv' if os.path.exists('/mnt/data/test.csv') else 'test.csv'\n",
    "\n",
    "if not os.path.exists(MNIST_TRAIN_PATH) or not os.path.exists(MNIST_TEST_PATH):\n",
    "    print('MNIST files missing!')\n",
    "else:\n",
    "    train = pd.read_csv(MNIST_TRAIN_PATH)\n",
    "    test  = pd.read_csv(MNIST_TEST_PATH)\n",
    "\n",
    "    # Separate features and labels\n",
    "    if 'label' in train.columns:\n",
    "        X_train = train.drop(columns=['label']).values / 255.0\n",
    "        y_train = train['label'].values\n",
    "    else:\n",
    "        X_train = train.iloc[:, 1:].values / 255.0\n",
    "        y_train = train.iloc[:, 0].values\n",
    "\n",
    "    if 'label' in test.columns:\n",
    "        X_test = test.drop(columns=['label']).values / 255.0\n",
    "        y_test = test['label'].values\n",
    "    else:\n",
    "        X_test = test.values / 255.0\n",
    "        y_test = None\n",
    "\n",
    "    print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1788d821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Accuracy = 0.8620\n",
      "Epoch 40: Train Accuracy = 0.8786\n",
      "Epoch 60: Train Accuracy = 0.8872\n",
      "Epoch 80: Train Accuracy = 0.8921\n",
      "Epoch 100: Train Accuracy = 0.8961\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Train Softmax Logistic Regression\n",
    "\n",
    "def one_hot(y, K):\n",
    "    oh = np.zeros((len(y), K))\n",
    "    oh[np.arange(len(y)), y] = 1\n",
    "    return oh\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - np.max(z, axis=1, keepdims=True)\n",
    "    ez = np.exp(z)\n",
    "    return ez / ez.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "W = np.zeros((X_train.shape[1], num_classes))\n",
    "b = np.zeros((1, num_classes))\n",
    "\n",
    "epochs = 100\n",
    "lr = 0.5\n",
    "\n",
    "y_oh = one_hot(y_train, num_classes)\n",
    "\n",
    "for ep in range(1, epochs + 1):\n",
    "    logits = X_train.dot(W) + b\n",
    "    probs = softmax(logits)\n",
    "    error = probs - y_oh\n",
    "    gradW = X_train.T.dot(error) / X_train.shape[0]\n",
    "    gradb = error.mean(axis=0, keepdims=True)\n",
    "    W -= lr * gradW\n",
    "    b -= lr * gradb\n",
    "\n",
    "    if ep % 20 == 0:\n",
    "        preds = np.argmax(softmax(X_train.dot(W) + b), axis=1)\n",
    "        acc = (preds == y_train).mean()\n",
    "        print(f\"Epoch {ep}: Train Accuracy = {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0c846fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No test labels found (y_test is None). Saving predictions to 'mnist_test_predictions.csv'.\n",
      "Saved mnist_test_predictions.csv in the current folder.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Evaluate MNIST Model (fixed)\n",
    "logits = X_test.dot(W) + b\n",
    "preds = np.argmax(softmax(logits), axis=1)\n",
    "\n",
    "# If test labels available, print metrics. Otherwise save predictions to CSV.\n",
    "if 'y_test' in globals() and y_test is not None:\n",
    "    # Ensure arrays are 1-D and same length\n",
    "    y_true_arr = np.array(y_test).reshape(-1)\n",
    "    preds_arr = np.array(preds).reshape(-1)\n",
    "    if y_true_arr.shape[0] != preds_arr.shape[0]:\n",
    "        print(f\"Mismatch in sizes: y_test has {y_true_arr.shape[0]} rows but preds has {preds_arr.shape[0]} rows.\")\n",
    "    else:\n",
    "        print_metrics_from_labels(y_true_arr, preds_arr, name=\"MNIST Logistic Regression\")\n",
    "else:\n",
    "    print(\"No test labels found (y_test is None). Saving predictions to 'mnist_test_predictions.csv'.\")\n",
    "    # If test had an 'id' column it could be included; otherwise just save index and predicted label\n",
    "    out_df = pd.DataFrame({'index': np.arange(len(preds)), 'predicted_label': preds})\n",
    "    out_df.to_csv('mnist_test_predictions.csv', index=False)\n",
    "    print(\"Saved mnist_test_predictions.csv in the current folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ccfe13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Display Sample Predictions\n",
    "\n",
    "def array_to_image(pixel_array):\n",
    "    plt.imshow(np.array(pixel_array).reshape(28, 28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def predict_and_show_from_array(pixel_array):\n",
    "    x = np.array(pixel_array).reshape(1, -1) / 255.0\n",
    "    probs = softmax(x.dot(W) + b)\n",
    "    label = int(np.argmax(probs))\n",
    "    array_to_image(pixel_array)\n",
    "    print(f\"Predicted Label: {label}, Confidence: {probs.max():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc1a0232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Target column not found! Check your CSV headers.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Load and Prepare Bank Dataset (Fixed with target detection)\n",
    "\n",
    "BANK_PATH = '/mnt/data/bank-full.csv' if os.path.exists('/mnt/data/bank-full.csv') else 'bank-full.csv'\n",
    "\n",
    "if not os.path.exists(BANK_PATH):\n",
    "    print('Bank dataset missing!')\n",
    "else:\n",
    "    # Load and shuffle\n",
    "    df = pd.read_csv(BANK_PATH).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Try to detect target column automatically\n",
    "    target_col = None\n",
    "    for possible in ['y', 'deposit', 'output', 'class', 'target']:\n",
    "        if possible in df.columns:\n",
    "            target_col = possible\n",
    "            break\n",
    "\n",
    "    if target_col is None:\n",
    "        print(\"❌ Target column not found! Check your CSV headers.\")\n",
    "    else:\n",
    "        print(f\"✅ Detected target column: '{target_col}'\")\n",
    "\n",
    "        # Split 80:20 for train/test\n",
    "        split = int(0.8 * len(df))\n",
    "        df_train = df.iloc[:split].reset_index(drop=True)\n",
    "        df_test = df.iloc[split:].reset_index(drop=True)\n",
    "\n",
    "        # Define numeric and categorical columns\n",
    "        numeric_cols = [c for c in ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous'] if c in df.columns]\n",
    "        categorical_cols = [c for c in df.columns if c not in numeric_cols + [target_col]]\n",
    "\n",
    "        # Map target labels to binary\n",
    "        label_map = {'no': 0, 'yes': 1}\n",
    "        y_train = df_train[target_col].map(label_map).values\n",
    "        y_test = df_test[target_col].map(label_map).values\n",
    "\n",
    "        print(f\"Train size: {len(df_train)}, Test size: {len(df_test)}\")\n",
    "        print(f\"Numeric columns: {numeric_cols}\")\n",
    "        print(f\"Sample categorical columns: {categorical_cols[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9592a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Cell 8: Train Naive Bayes Model\n",
    "\n",
    "\n",
    "# %%\n",
    "classes = np.unique(y_train)\n",
    "prior, cont_stats, cat_counts = {}, {c:{} for c in classes}, {c:{} for c in classes}\n",
    "for c in classes:\n",
    "sub = df_train[df_train['y'].map(label_map)==c]\n",
    "prior[c] = len(sub)/len(df_train)\n",
    "for col in numeric_cols:\n",
    "vals = sub[col].astype(float).values\n",
    "mu, var = vals.mean(), vals.var(ddof=0) if vals.var()!=0 else 1e-6\n",
    "cont_stats[c][col] = (mu,var)\n",
    "for col in categorical_cols:\n",
    "cat_counts[c][col] = sub[col].astype(str).value_counts().to_dict()\n",
    "\n",
    "\n",
    "vocab = {col: pd.concat([df_train[col].astype(str), df_test[col].astype(str)]).unique().tolist() for col in categorical_cols}\n",
    "\n",
    "\n",
    "def log_gauss(x, mu, var):\n",
    "return -0.5*(math.log(2*math.pi*var)+((x-mu)**2)/var)\n",
    "\n",
    "\n",
    "def nb_predict_row(row):\n",
    "best, bestlp = None, -1e18\n",
    "for c in classes:\n",
    "lp = math.log(prior[c]+1e-12)\n",
    "for col in numeric_cols:\n",
    "mu,var = cont_stats[c][col]\n",
    "lp += log_gauss(float(row[col]), mu, var)\n",
    "for col in categorical_cols:\n",
    "v = str(row[col])\n",
    "counts = cat_counts[c].get(col, {})\n",
    "cnt = counts.get(v,0)\n",
    "total = sum(counts.values())\n",
    "V = len(vocab[col])\n",
    "prob = (cnt+1)/(total+V)\n",
    "lp += math.log(prob+1e-12)\n",
    "if lp>bestlp:\n",
    "bestlp, best = lp, c\n",
    "return int(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2eee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Cell 9: Evaluate Naive Bayes\n",
    "\n",
    "\n",
    "# %%\n",
    "y_pred_nb = np.array([nb_predict_row(row) for _,row in df_test.iterrows()])\n",
    "print_metrics_from_labels(y_test, y_pred_nb, labels=[0,1], name=\"Naive Bayes (bank-full)\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Cell 10: Display Priors and Example Stats\n",
    "\n",
    "\n",
    "# %%\n",
    "for c in classes:\n",
    "print(f\"Class {c} prior: {prior[c]:.4f}\")\n",
    "for col in numeric_cols[:4]:\n",
    "mu,var = cont_stats[1][col]\n",
    "print(f\"{col}: mean={mu:.3f}, var={var:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
